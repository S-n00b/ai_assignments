# Small Models Configuration for Mobile/Edge Deployment
# Phase 2: Model Integration & Small Model Selection

small_models:
  # Optimized small models for mobile/edge deployment (<4B parameters)
  phi-4-mini:
    provider: microsoft
    parameters: 3.8B
    github_models_id: microsoft/phi-4-mini-instruct
    ollama_name: phi4-mini
    use_case: mobile_edge_deployment
    fine_tuning_target: true
    size_gb: 2.3
    memory_requirements:
      minimum: 4
      recommended: 6
    performance_profile:
      latency_ms: 150
      throughput_tokens_per_sec: 45
      accuracy_score: 0.85
    deployment_targets:
      - mobile
      - edge
      - embedded
    optimization_flags:
      - quantization
      - pruning
      - distillation

  llama-3.2-3b:
    provider: meta
    parameters: 3B
    github_models_id: meta/llama-3.2-3b-instruct
    ollama_name: llama3.2:3b
    use_case: on_device_inference
    fine_tuning_target: true
    size_gb: 1.9
    memory_requirements:
      minimum: 3
      recommended: 5
    performance_profile:
      latency_ms: 120
      throughput_tokens_per_sec: 50
      accuracy_score: 0.82
    deployment_targets:
      - mobile
      - edge
      - embedded
    optimization_flags:
      - quantization
      - pruning

  qwen-2.5-3b:
    provider: alibaba
    parameters: 3B
    github_models_id: alibaba/qwen-2.5-3b-instruct
    ollama_name: qwen2.5:3b
    use_case: chinese_mobile_support
    fine_tuning_target: true
    size_gb: 1.9
    memory_requirements:
      minimum: 3
      recommended: 5
    performance_profile:
      latency_ms: 130
      throughput_tokens_per_sec: 48
      accuracy_score: 0.84
    deployment_targets:
      - mobile
      - edge
      - embedded
    optimization_flags:
      - quantization
      - multilingual_optimization

  mistral-nemo:
    provider: mistral
    parameters: 3B
    github_models_id: mistral/mistral-nemo
    ollama_name: nemo
    use_case: efficient_mobile_ai
    fine_tuning_target: true
    size_gb: 1.8
    memory_requirements:
      minimum: 3
      recommended: 4
    performance_profile:
      latency_ms: 110
      throughput_tokens_per_sec: 55
      accuracy_score: 0.83
    deployment_targets:
      - mobile
      - edge
      - embedded
    optimization_flags:
      - quantization
      - efficient_attention

# GitHub Models API Configuration
github_models_api:
  base_url: https://models.github.ai
  organization: Brantwood
  authentication:
    token_env_var: GITHUB_MODELS_TOKEN
    scopes:
      - models:read
  rate_limits:
    requests_per_hour: 5000
    requests_per_minute: 100
  inference_endpoints:
    chat_completions: /inference/chat/completions
    org_chat_completions: /inference/orgs/{org}/chat/completions
    model_catalog: /catalog/models
  api_version: "2022-11-28"
  supported_models:
    - openai/gpt-4.1
    - microsoft/phi-4-mini
    - meta/llama-3.2-3b
    - alibaba/qwen-2.5-3b
    - mistral/mistral-nemo

# Ollama Integration Configuration
ollama_integration:
  small_model_optimization:
    enable_quantization: true
    quantization_method: q4_0
    enable_pruning: true
    pruning_ratio: 0.1
    enable_distillation: true
    teacher_model: llama3.1:8b

  mobile_deployment_configs:
    android:
      target_arch: arm64-v8a
      optimization_level: O3
      memory_pool_size: 512MB
    ios:
      target_arch: arm64
      optimization_level: O3
      memory_pool_size: 256MB
    edge:
      target_arch: x86_64
      optimization_level: O2
      memory_pool_size: 1GB

  performance_monitoring:
    metrics_collection_interval: 5
    latency_tracking: true
    memory_usage_tracking: true
    gpu_utilization_tracking: false
    alert_thresholds:
      latency_ms: 500
      memory_usage_percent: 80
      cpu_usage_percent: 85

# Model Evaluation Configuration
model_evaluation:
  small_model_benchmarks:
    - name: mobile_inference_speed
      description: "Speed test for mobile inference"
      metrics:
        - latency_ms
        - throughput_tokens_per_sec
        - memory_usage_mb
    - name: edge_deployment_test
      description: "Edge deployment compatibility test"
      metrics:
        - deployment_success
        - resource_utilization
        - stability_score
    - name: accuracy_benchmark
      description: "Accuracy comparison with larger models"
      metrics:
        - perplexity
        - bleu_score
        - rouge_score
        - human_evaluation_score

# Factory Roster Integration
factory_roster:
  small_model_deployment:
    production_targets:
      - lenovo_moto_edge_series
      - lenovo_thinkpad_mobile
      - lenovo_iot_devices
    quality_gates:
      - minimum_accuracy_threshold: 0.80
      - maximum_latency_ms: 300
      - maximum_memory_mb: 1024
      - stability_test_hours: 24
    deployment_pipeline:
      - model_validation
      - performance_testing
      - integration_testing
      - production_deployment
      - monitoring_setup
